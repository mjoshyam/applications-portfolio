{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjoshyam/applications-portfolio/blob/main/emotion_dataset_persona_builder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezNo1CKe10Vh"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Jan  2 21:17:52 2026"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YQuOyjx10Vk"
      },
      "outputs": [],
      "source": [
        "@author: Daku\n",
        "\"\"\"\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gxl5kaN10Vk"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import hstack, csr_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmtco9PU10Vl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPrnzpnV10Vl"
      },
      "source": [
        "--------------------------Loading and cleaning the original Kaggle dataset----------------------#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXBvShGN10Vm"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"/Users/Daku/Desktop/OpenAI_Residency/Datasets/emotion_sentimen_dataset.csv\"\n",
        "df = pd.read_csv(DATA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWVs5bVU10Vm"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
        "df = df.dropna(subset=[\"text\", \"Emotion\"]).copy()\n",
        "df[\"text\"] = df[\"text\"].astype(str).str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXX_Efus10Vn"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "print(df[\"Emotion\"].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC0oxZ1T10Vn"
      },
      "source": [
        "---------------Loading the csv containing digitized flash cards from \"Box of Emotions\"-----------# <br>\n",
        "------------------------The cards will be treated as theory anchors------------------------------#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjczHQ1f10Vn"
      },
      "outputs": [],
      "source": [
        "CARDS_PATH = \"/Users/Daku/Desktop/OpenAI_Residency/Datasets/cards.csv\"\n",
        "cards = pd.read_csv(CARDS_PATH).dropna(subset=[\"card_name\", \"definition\"]).copy()\n",
        "cards[\"card_name\"] = cards[\"card_name\"].astype(str).str.strip()\n",
        "cards[\"system\"] = cards[\"system\"].astype(str).str.strip()\n",
        "cards[\"definition\"] = cards[\"definition\"].astype(str).str.strip()\n",
        "print(cards.shape)\n",
        "print(cards[\"system\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oPDLmU010Vo"
      },
      "source": [
        "------------------Vectorize the card definitions into a card embeddings space--------------------#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdNRn4RF10Vo"
      },
      "outputs": [],
      "source": [
        "card_vec = TfidfVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1,2),\n",
        "    min_df=1\n",
        ")\n",
        "C = card_vec.fit_transform(cards[\"definition\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIRnLHRM10Vo"
      },
      "source": [
        "------------Create psychological and behavioral cue features from text-----------------------# <br>\n",
        "Simple lexicons (TODO: expand this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B9Mcz1g10Vo"
      },
      "outputs": [],
      "source": [
        "NEGATIONS = set([\"not\",\"no\",\"never\",\"none\",\"nothing\",\"n't\"])\n",
        "MODALS = set([\"should\",\"must\",\"need\",\"have to\",\"can't\",\"cannot\",\"could\",\"might\",\"may\"])\n",
        "TIME_PAST = re.compile(r\"\\b(was|were|had|did|ago|yesterday|before)\\b\", re.I)\n",
        "TIME_FUTURE = re.compile(r\"\\b(will|gonna|going to|tomorrow|next|soon|might)\\b\", re.I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLcfmP2v10Vo"
      },
      "outputs": [],
      "source": [
        "MORAL = re.compile(r\"\\b(deserve|fault|blame|wrong|should|ought|fair|unfair|shame|guilt)\\b\", re.I)\n",
        "THREAT = re.compile(r\"\\b(threat|danger|scared|fear|panic|terrified|unsafe)\\b\", re.I)\n",
        "LOSS = re.compile(r\"\\b(miss|lost|gone|grief|sad|lonely|heartbroken)\\b\", re.I)\n",
        "COMPARE = re.compile(r\"\\b(better|worse|than|others|they have|why them)\\b\", re.I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyKh6tb_10Vo"
      },
      "outputs": [],
      "source": [
        "def cue_features(text: str) -> np.ndarray:\n",
        "    t = text.lower()\n",
        "    tokens = re.findall(r\"[a-z']+\", t)\n",
        "    n = max(len(tokens), 1)\n",
        "\n",
        "    # pronoun focus\n",
        "    i_cnt = sum(tok in (\"i\",\"me\",\"my\",\"mine\") for tok in tokens)\n",
        "    you_cnt = sum(tok in (\"you\",\"your\",\"yours\") for tok in tokens)\n",
        "    they_cnt = sum(tok in (\"they\",\"them\",\"their\",\"theirs\") for tok in tokens)\n",
        "\n",
        "    # simple counts\n",
        "    neg = sum(tok in NEGATIONS for tok in tokens)\n",
        "    exclam = text.count(\"!\")\n",
        "    ques = text.count(\"?\")\n",
        "    caps = sum(1 for ch in text if ch.isalpha() and ch.isupper())\n",
        "    alpha = sum(1 for ch in text if ch.isalpha())\n",
        "    caps_ratio = caps / max(alpha, 1)\n",
        "\n",
        "    # pattern hits\n",
        "    past = 1 if TIME_PAST.search(text) else 0\n",
        "    future = 1 if TIME_FUTURE.search(text) else 0\n",
        "    moral = 1 if MORAL.search(text) else 0\n",
        "    threat = 1 if THREAT.search(text) else 0\n",
        "    loss = 1 if LOSS.search(text) else 0\n",
        "    compare = 1 if COMPARE.search(text) else 0\n",
        "    return np.array([\n",
        "        len(tokens),                 # length\n",
        "        i_cnt / n, you_cnt / n, they_cnt / n,\n",
        "        neg / n,\n",
        "        exclam, ques,\n",
        "        caps_ratio,\n",
        "        past, future,\n",
        "        moral, threat, loss, compare\n",
        "    ], dtype=float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQk_2jx010Vp"
      },
      "outputs": [],
      "source": [
        "def build_cue_matrix(texts: pd.Series, batch_size=50000) -> csr_matrix:\n",
        "    feats = []\n",
        "    for start in tqdm(range(0, len(texts), batch_size)):\n",
        "        chunk = texts.iloc[start:start+batch_size]\n",
        "        arr = np.vstack([cue_features(x) for x in chunk])\n",
        "        feats.append(csr_matrix(arr))\n",
        "    return csr_matrix(np.vstack([f.toarray() for f in feats]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkRLiMSF10Vp"
      },
      "source": [
        "For a first run, sample (scale up after debugging)<br>\n",
        "sample = df.sample(n=min(200000, len(df)), random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IFR2AR810Vp"
      },
      "outputs": [],
      "source": [
        "X_cues = build_cue_matrix(df[\"text\"])\n",
        "print(X_cues.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrXuuBK-10Vp"
      },
      "source": [
        "-----Learning expression of emotions (Linguistic Expression Model)------------#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpDlaOsX10Vp"
      },
      "outputs": [],
      "source": [
        "text_vec = HashingVectorizer(\n",
        "    n_features=2**20,\n",
        "    alternate_sign=False,\n",
        "    ngram_range=(1,2),\n",
        "    token_pattern=r\"(?u)\\b[\\w']+\\b\"\n",
        ")\n",
        "X_text = text_vec.transform(df[\"text\"])\n",
        "y = df[\"Emotion\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbJ6v_Nx10Vp"
      },
      "outputs": [],
      "source": [
        "X = hstack([X_text, X_cues]).tocsr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mixx7Gk710Vp"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr5uCHen10Vp"
      },
      "outputs": [],
      "source": [
        "clf = SGDClassifier(loss=\"log_loss\", alpha=1e-6, max_iter=200, n_jobs=-1)\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oBhr-Rt10Vp"
      },
      "outputs": [],
      "source": [
        "print(\"Baseline emotion model trained.\")\n",
        "print(\"Test accuracy (rough):\", clf.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g1uuAum10Vp"
      },
      "source": [
        "----------------Interrogate dataset against card theory---------------------#<br>\n",
        "a) Build card similarity score on each text <br>\n",
        "Card-space vectorizer: fit on (cards + data sample) so vocabulary covers both"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxKQIVNf10Vp"
      },
      "outputs": [],
      "source": [
        "card_space_vec = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), min_df=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwraR4Px10Vp"
      },
      "outputs": [],
      "source": [
        "combined = pd.concat([cards[\"definition\"], df[\"text\"]], ignore_index=True)\n",
        "card_space_vec.fit(combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3QyMwpP10Vq"
      },
      "outputs": [],
      "source": [
        "C = card_space_vec.transform(cards[\"definition\"])\n",
        "T = card_space_vec.transform(df[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxH33SJI10Vq"
      },
      "source": [
        "Similarity: each text gets similarity to each card definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3PA8wXm10Vq"
      },
      "outputs": [],
      "source": [
        "S = cosine_similarity(T, C) # this cannot be done for the full dataset as this\n",
        "# breaks and needs chunking to avoid S vector to have ~63 million floats\n",
        "print(\"Cosing similaraity completed!\")\n",
        "# --------Extract top-k card lenses per text (the theory projection)-----#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1qz2IgW10Vq"
      },
      "outputs": [],
      "source": [
        "card_names = cards[\"card_name\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIQaa_QZ10Vq"
      },
      "outputs": [],
      "source": [
        "TOPK = 3\n",
        "topk_idx = np.argsort(-S, axis=1)[:, :TOPK]\n",
        "topk_cards = [[card_names[j] for j in row] for row in topk_idx]\n",
        "topk_scores = np.take_along_axis(S, topk_idx, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDgylYkp10Vq"
      },
      "outputs": [],
      "source": [
        "df[\"theory_cards_top3\"] = [\";\".join(x) for x in topk_cards]\n",
        "df[\"theory_top1\"] = [x[0] for x in topk_cards]\n",
        "df[\"theory_top1_score\"] = topk_scores[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooYHeqBc10Vq"
      },
      "outputs": [],
      "source": [
        "df[[\"Emotion\", \"theory_top1\", \"theory_top1_score\"]].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH9ZOMMQ10Vq"
      },
      "source": [
        "ecompose expressions into psychological and behavioral cues (structured inference)------------#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K3hUx1f10Vq"
      },
      "outputs": [],
      "source": [
        "cue_cols = [\n",
        "    \"len_tokens\",\"i_ratio\",\"you_ratio\",\"they_ratio\",\"neg_ratio\",\n",
        "    \"exclam\",\"ques\",\"caps_ratio\",\"past\",\"future\",\"moral\",\"threat\",\"loss\",\"compare\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xbKk4RT10Vq"
      },
      "outputs": [],
      "source": [
        "cue_df = pd.DataFrame(X_cues.toarray(), columns=cue_cols)\n",
        "tmp = pd.concat([df[[\"Emotion\",\"theory_top1\"]], cue_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBPTmLRR10Vq"
      },
      "outputs": [],
      "source": [
        "summary = tmp.groupby([\"Emotion\",\"theory_top1\"])[cue_cols].mean().sort_values(\"threat\", ascending=False)\n",
        "summary.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4EpRBxJ10Vw"
      },
      "source": [
        "--------Build personas from mechanisms-------------------#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6gBn8P610Vw"
      },
      "source": [
        "---- a) Create a “persona feature matrix-----------# <br>\n",
        "Reduce the 79-d theory similarity into smaller dimensions for clustering stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR51sCUt10Vw"
      },
      "outputs": [],
      "source": [
        "svd = TruncatedSVD(n_components=25, random_state=42)\n",
        "S_reduced = svd.fit_transform(S)  # [N, 25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgGMYp0n10Vw"
      },
      "outputs": [],
      "source": [
        "X_persona = hstack([X_cues, csr_matrix(S_reduced)]).tocsr()\n",
        "print(X_persona.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T0LHDUK10Vw"
      },
      "source": [
        "---- b) Cluster into Personas -------------------#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcJqMfS410Vw"
      },
      "outputs": [],
      "source": [
        "k = 8  # start with 6–10; tune later via stability + interpretability\n",
        "km = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=4096, n_init=\"auto\")\n",
        "persona_id = km.fit_predict(X_persona)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usqACG5310Vw"
      },
      "outputs": [],
      "source": [
        "df[\"persona_id\"] = persona_id\n",
        "df[\"persona_id\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sR14zbn10Vw"
      },
      "source": [
        "-------Make Personas Interpretable---------------#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1tu8Ye-10Vw"
      },
      "source": [
        "-------naming personas using top theory cards, cue profiles and texts----#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "got4iZtB10Vx"
      },
      "outputs": [],
      "source": [
        "def top_items(series, n=5):\n",
        "    return series.value_counts().head(n).to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3ZhYTzt10Vx"
      },
      "outputs": [],
      "source": [
        "persona_reports = []\n",
        "for pid in sorted(df[\"persona_id\"].unique()):\n",
        "    sub = df[df[\"persona_id\"] == pid]\n",
        "\n",
        "    # Top theory lenses\n",
        "    top_theory = top_items(sub[\"theory_top1\"], n=7)\n",
        "\n",
        "    # Cue means\n",
        "    cue_means = cue_df.loc[sub.index].mean().to_dict()\n",
        "\n",
        "    # Representative examples (highest top1 score)\n",
        "    ex = sub.sort_values(\"theory_top1_score\", ascending=False).head(3)[\"text\"].tolist()\n",
        "\n",
        "    persona_reports.append({\n",
        "        \"persona_id\": pid,\n",
        "        \"size\": len(sub),\n",
        "        \"top_theory_cards\": top_theory,\n",
        "        \"cue_means\": cue_means,\n",
        "        \"examples\": ex\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vEY8FHL10Vx"
      },
      "outputs": [],
      "source": [
        "persona_reports[0][\"top_theory_cards\"], persona_reports[0][\"examples\"][:1]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}