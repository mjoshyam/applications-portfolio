{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjoshyam/applications-portfolio/blob/main/emotion_dataset_persona_builder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ezNo1CKe10Vh",
        "outputId": "6816a131-8e9c-46aa-fe9f-982b02b375fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCreated on Fri Jan  2 21:17:52 2026'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Jan  2 21:17:52 2026\"\"\"\n",
        "# Author: Manasa Joshyam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Imports"
      ],
      "metadata": {
        "id": "qrSkeD6ytjWz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5YQuOyjx10Vk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPrnzpnV10Vl"
      },
      "source": [
        "# 2. Load and clean the original dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3Kgljah_t8as"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TXBvShGN10Vm",
        "outputId": "0034acfb-bf03-404f-9945-739739c4fccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(839555, 2)\n",
            "Emotion\n",
            "neutral       674538\n",
            "love           39553\n",
            "happiness      27175\n",
            "sadness        17481\n",
            "relief         16729\n",
            "hate           15267\n",
            "anger          12336\n",
            "fun            10075\n",
            "enthusiasm      9304\n",
            "surprise        6954\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = \"/content/emotion_sentimen_dataset.csv\"\n",
        "emotion_ds = pd.read_csv(DATA_PATH)\n",
        "# dropping unnamed columns in original dataset\n",
        "emotion_ds = emotion_ds.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
        "# dropping NA values in the text and emotion columns\n",
        "emotion_ds = emotion_ds.dropna(subset=[\"text\", \"Emotion\"]).copy()\n",
        "# enforcing type safety for string and removing trailing and leading spacers, new lines, tabs, etc.\n",
        "emotion_ds[\"text\"] = emotion_ds[\"text\"].astype(str).str.strip()\n",
        "print(emotion_ds.shape)\n",
        "print(emotion_ds[\"Emotion\"].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC0oxZ1T10Vn"
      },
      "source": [
        "# 3. Theory Anchors\n",
        "\n",
        "Loading the csv containing digitized flash cards from \"Box of Emotions\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjczHQ1f10Vn",
        "outputId": "43686f24-e129-4d75-879a-b8883f2b7f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(68, 18)\n",
            "system\n",
            "Heartache      8\n",
            "Angst          7\n",
            "Heat           7\n",
            "Enjoyment      6\n",
            "Loathing       6\n",
            "Bitterness     6\n",
            "Ego            6\n",
            "Zen            6\n",
            "Bliss          6\n",
            "Emptiness      5\n",
            "Possibility    5\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "CARDS_PATH = \"/content/cards_v1_cleaned_full.csv\"\n",
        "cards = pd.read_csv(CARDS_PATH).dropna(subset=[\"card_name\", \"definition\"]).copy()\n",
        "# enforcing type safety for string and removing trailing and leading spacers, new lines, tabs, etc.\n",
        "cards[\"card_name\"] = cards[\"card_name\"].astype(str).str.strip()\n",
        "cards[\"system\"] = cards[\"system\"].astype(str).str.strip()\n",
        "cards[\"definition\"] = cards[\"definition\"].astype(str).str.strip()\n",
        "print(cards.shape)\n",
        "print(cards[\"system\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oPDLmU010Vo"
      },
      "source": [
        "# 4. Vectorize the card definitions into a card embeddings space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YdNRn4RF10Vo"
      },
      "outputs": [],
      "source": [
        "# Construct a numeric sparse matrix of word importance. Each element in the matrix is a number that is a product of TF and IDF.\n",
        "# TF (Term Frequency) score designates how often a word occurs\n",
        "# IDF (Inverse Document Frequency) score designates how rare the occurrence is in other documents\n",
        "# Intuition: TF x IDF high score: The word is frequent in one document but rare everywhere else (a \"signature\" word).\n",
        "card_vec = TfidfVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1,2),\n",
        "    min_df=1\n",
        ")\n",
        "C = card_vec.fit_transform(cards[\"definition\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIRnLHRM10Vo"
      },
      "source": [
        "\n",
        "# 5. Psychological/Behavioral Cue Features from Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "1B9Mcz1g10Vo"
      },
      "outputs": [],
      "source": [
        "# Simple lexicons that can be expanded\n",
        "\n",
        "NEGATIONS = set([\"not\",\"no\",\"never\",\"none\",\"nothing\",\"n't\"])\n",
        "MODALS = set([\"should\",\"must\",\"need\",\"have to\",\"can't\",\"cannot\",\"could\",\"might\",\"may\"])\n",
        "TIME_PAST = re.compile(r\"\\b(was|were|had|did|ago|yesterday|before)\\b\", re.I)\n",
        "TIME_FUTURE = re.compile(r\"\\b(will|gonna|going to|tomorrow|next|soon|might)\\b\", re.I)\n",
        "\n",
        "MORAL = re.compile(r\"\\b(deserve|fault|blame|wrong|should|ought|fair|unfair|shame|guilt)\\b\", re.I)\n",
        "THREAT = re.compile(r\"\\b(threat|danger|scared|fear|panic|terrified|unsafe)\\b\", re.I)\n",
        "LOSS = re.compile(r\"\\b(miss|lost|gone|grief|sad|lonely|heartbroken)\\b\", re.I)\n",
        "COMPARE = re.compile(r\"\\b(better|worse|than|others|they have|why them)\\b\", re.I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ZyKh6tb_10Vo"
      },
      "outputs": [],
      "source": [
        "def cue_features(text: str) -> np.ndarray:\n",
        "    t = text.lower()\n",
        "    tokens = re.findall(r\"[a-z']+\", t)\n",
        "    n = max(len(tokens), 1)\n",
        "\n",
        "    # pronoun focus\n",
        "    i_cnt = sum(tok in (\"i\",\"me\",\"my\",\"mine\") for tok in tokens)\n",
        "    you_cnt = sum(tok in (\"you\",\"your\",\"yours\") for tok in tokens)\n",
        "    they_cnt = sum(tok in (\"they\",\"them\",\"their\",\"theirs\") for tok in tokens)\n",
        "\n",
        "    # simple counts\n",
        "    neg = sum(tok in NEGATIONS for tok in tokens)\n",
        "    exclam = text.count(\"!\")\n",
        "    ques = text.count(\"?\")\n",
        "    caps = sum(1 for ch in text if ch.isalpha() and ch.isupper())\n",
        "    alpha = sum(1 for ch in text if ch.isalpha())\n",
        "    caps_ratio = caps / max(alpha, 1)\n",
        "\n",
        "    # pattern hits\n",
        "    past = 1 if TIME_PAST.search(text) else 0\n",
        "    future = 1 if TIME_FUTURE.search(text) else 0\n",
        "    moral = 1 if MORAL.search(text) else 0\n",
        "    threat = 1 if THREAT.search(text) else 0\n",
        "    loss = 1 if LOSS.search(text) else 0\n",
        "    compare = 1 if COMPARE.search(text) else 0\n",
        "    return np.array([\n",
        "        len(tokens),                 # length\n",
        "        i_cnt / n, you_cnt / n, they_cnt / n,\n",
        "        neg / n,\n",
        "        exclam, ques,\n",
        "        caps_ratio,\n",
        "        past, future,\n",
        "        moral, threat, loss, compare\n",
        "    ], dtype=float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IFR2AR810Vp",
        "outputId": "dbd06aec-5e15-4593-b0ad-e9f40bb12b58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [01:06<00:00,  3.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(839555, 14)\n"
          ]
        }
      ],
      "source": [
        "def build_cue_matrix(texts: pd.Series, batch_size=50000) -> csr_matrix:\n",
        "    feats = []\n",
        "    for start in tqdm(range(0, len(texts), batch_size)):\n",
        "        chunk = texts.iloc[start:start+batch_size]\n",
        "        arr = np.vstack([cue_features(x) for x in chunk])\n",
        "        feats.append(csr_matrix(arr))\n",
        "    return csr_matrix(np.vstack([f.toarray() for f in feats]))\n",
        "\n",
        "# For a first run, sample (scale up after debugging)\n",
        "#sample = emotion_ds.sample(n=min(200000, len(emotion_ds)), random_state=42).reset_index(drop=True)\n",
        "X_cues = build_cue_matrix(emotion_ds[\"text\"])\n",
        "print(X_cues.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrXuuBK-10Vp"
      },
      "source": [
        "# 6. Linguistic Expression Model\n",
        "Learning how emotions are expressed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpDlaOsX10Vp",
        "outputId": "50713c58-b94c-40a4-ece3-ae51cab192e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline emotion model trained.\n",
            "Test accuracy (rough): 0.9652434920880705\n"
          ]
        }
      ],
      "source": [
        "text_vec = HashingVectorizer(\n",
        "    n_features=2**20,\n",
        "    alternate_sign=False,\n",
        "    ngram_range=(1,2),\n",
        "    token_pattern=r\"(?u)\\b[\\w']+\\b\"\n",
        ")\n",
        "X_text = text_vec.transform(emotion_ds[\"text\"])\n",
        "y = emotion_ds[\"Emotion\"].values\n",
        "\n",
        "X = hstack([X_text, X_cues]).tocsr()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "clf = SGDClassifier(loss=\"log_loss\", alpha=1e-6, max_iter=1000, n_jobs=-1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Baseline emotion model trained.\")\n",
        "print(\"Test accuracy (rough):\", clf.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g1uuAum10Vp"
      },
      "source": [
        "# 7. Use cards as a theory lens to interpret texts and errors\n",
        "\n",
        "7A) Build a card similarity score for each text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Card-space vectorizer: fit on (cards + data sample) so vocabulary covers both\n",
        "card_space_vec = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), min_df=5)\n",
        "\n",
        "combined = pd.concat([cards[\"definition\"], emotion_ds[\"text\"]], ignore_index=True)\n",
        "card_space_vec.fit(combined)\n",
        "\n",
        "C = card_space_vec.transform(cards[\"definition\"])  # [79, V]\n",
        "T = card_space_vec.transform(emotion_ds[\"text\"])       # [N, V]\n",
        "\n",
        "# Similarity: each text gets similarity to each card definition\n",
        "S = cosine_similarity(T, C)  # [N, 79]"
      ],
      "metadata": {
        "id": "Yeu9oJVdf3tq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK172ikkVN3M",
        "outputId": "234882b7-999c-4321-8b37-5fb203a88d5a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(839555, 68)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxH33SJI10Vq"
      },
      "source": [
        "7B) Extract top-k card lenses per text (the “theory projection”)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3PA8wXm10Vq",
        "outputId": "d2670753-6a07-415a-9d9a-20fa85e3616c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Emotion theory_top1  theory_top1_score\n",
            "0     hate      Hatred           0.046123\n",
            "1  neutral        Fago           0.024254\n",
            "2  neutral      Regret           0.045607\n",
            "3    anger     Disgust           0.020991\n",
            "4  neutral   Self-Pity           0.044900\n",
            "5     love        Love           0.040288\n",
            "6  neutral         Zal           0.058141\n",
            "7    worry     Anxiety           0.106512\n",
            "8  neutral  Impatience           0.054342\n",
            "9  neutral        Fago           0.041550\n"
          ]
        }
      ],
      "source": [
        "card_names = cards[\"card_name\"].tolist()\n",
        "\n",
        "TOPK = 3\n",
        "topk_idx = np.argsort(-S, axis=1)[:, :TOPK]\n",
        "topk_cards = [[card_names[j] for j in row] for row in topk_idx]\n",
        "topk_scores = np.take_along_axis(S, topk_idx, axis=1)\n",
        "\n",
        "emotion_ds[\"theory_cards_top3\"] = [\";\".join(x) for x in topk_cards]\n",
        "emotion_ds[\"theory_top1\"] = [x[0] for x in topk_cards]\n",
        "emotion_ds[\"theory_top1_score\"] = topk_scores[:,0]\n",
        "\n",
        "print(emotion_ds[[\"Emotion\", \"theory_top1\", \"theory_top1_score\"]].head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topk_idx.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV_u1k3CZLEg",
        "outputId": "822acecf-3a75-482f-f414-59e84abc95ba"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(839555, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH9ZOMMQ10Vq"
      },
      "source": [
        "# 8) Decompose expressions into psychological / behavioral cues\n",
        "\n",
        "(structured inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "1K3hUx1f10Vq",
        "outputId": "127fda29-4ac1-440d-f2af-62fc29f39eed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         len_tokens   i_ratio  you_ratio  they_ratio  \\\n",
              "Emotion   theory_top1                                                  \n",
              "boredom   Melancholy      38.000000  0.131579   0.000000    0.000000   \n",
              "worry     Guilt           30.076923  0.072215   0.003205    0.003846   \n",
              "love      Guilt           26.885135  0.111495   0.006719    0.004642   \n",
              "sadness   Guilt           22.397959  0.112080   0.003088    0.003618   \n",
              "anger     Guilt           22.467532  0.109618   0.008466    0.004560   \n",
              "happiness Guilt           25.900585  0.118138   0.002961    0.003329   \n",
              "worry     Shame           28.933333  0.107037   0.007255    0.012605   \n",
              "surprise  Guilt           27.850000  0.110769   0.003590    0.007568   \n",
              "hate      Guilt           25.264901  0.112590   0.008743    0.003770   \n",
              "empty     Panic           22.555556  0.097131   0.010582    0.003584   \n",
              "worry     Schadenfreude   24.000000  0.121501   0.000000    0.012107   \n",
              "fun       Guilt           24.449612  0.081840   0.004261    0.006329   \n",
              "worry     Pride           33.200000  0.104345   0.000000    0.000000   \n",
              "hate      Pride           32.900000  0.104197   0.008873    0.010459   \n",
              "neutral   Guilt           19.159732  0.122145   0.003853    0.004643   \n",
              "empty     Guilt           26.288136  0.091703   0.004039    0.003110   \n",
              "worry     Irritation      31.272727  0.090623   0.001855    0.005172   \n",
              "relief    Guilt           27.477612  0.091336   0.006002    0.013307   \n",
              "love      Shame           27.817694  0.113335   0.011035    0.007066   \n",
              "hate      Boredom         32.106667  0.092295   0.013706    0.003206   \n",
              "\n",
              "                         neg_ratio  exclam  ques  caps_ratio      past  \\\n",
              "Emotion   theory_top1                                                    \n",
              "boredom   Melancholy      0.000000     0.0   0.0         0.0  1.000000   \n",
              "worry     Guilt           0.014251     0.0   0.0         0.0  0.230769   \n",
              "love      Guilt           0.009820     0.0   0.0         0.0  0.189189   \n",
              "sadness   Guilt           0.010459     0.0   0.0         0.0  0.183673   \n",
              "anger     Guilt           0.008041     0.0   0.0         0.0  0.168831   \n",
              "happiness Guilt           0.010116     0.0   0.0         0.0  0.172515   \n",
              "worry     Shame           0.012093     0.0   0.0         0.0  0.066667   \n",
              "surprise  Guilt           0.009208     0.0   0.0         0.0  0.300000   \n",
              "hate      Guilt           0.009084     0.0   0.0         0.0  0.145695   \n",
              "empty     Panic           0.000000     0.0   0.0         0.0  0.000000   \n",
              "worry     Schadenfreude   0.003175     0.0   0.0         0.0  0.222222   \n",
              "fun       Guilt           0.014536     0.0   0.0         0.0  0.170543   \n",
              "worry     Pride           0.013462     0.0   0.0         0.0  0.200000   \n",
              "hate      Pride           0.008372     0.0   0.0         0.0  0.125000   \n",
              "neutral   Guilt           0.011576     0.0   0.0         0.0  0.147900   \n",
              "empty     Guilt           0.005869     0.0   0.0         0.0  0.169492   \n",
              "worry     Irritation      0.004132     0.0   0.0         0.0  0.272727   \n",
              "relief    Guilt           0.012994     0.0   0.0         0.0  0.179104   \n",
              "love      Shame           0.010076     0.0   0.0         0.0  0.238606   \n",
              "hate      Boredom         0.014235     0.0   0.0         0.0  0.386667   \n",
              "\n",
              "                           future     moral    threat      loss   compare  \n",
              "Emotion   theory_top1                                                      \n",
              "boredom   Melancholy     0.000000  1.000000  0.000000  0.000000  1.000000  \n",
              "worry     Guilt          0.153846  0.615385  0.000000  0.000000  0.000000  \n",
              "love      Guilt          0.060811  0.310811  0.006757  0.027027  0.067568  \n",
              "sadness   Guilt          0.061224  0.275510  0.020408  0.530612  0.061224  \n",
              "anger     Guilt          0.038961  0.272727  0.012987  0.025974  0.090909  \n",
              "happiness Guilt          0.093567  0.269006  0.000000  0.040936  0.099415  \n",
              "worry     Shame          0.066667  0.266667  0.133333  0.000000  0.066667  \n",
              "surprise  Guilt          0.050000  0.250000  0.000000  0.050000  0.000000  \n",
              "hate      Guilt          0.092715  0.238411  0.000000  0.000000  0.066225  \n",
              "empty     Panic          0.111111  0.222222  0.000000  0.000000  0.000000  \n",
              "worry     Schadenfreude  0.111111  0.222222  0.000000  0.000000  0.111111  \n",
              "fun       Guilt          0.093023  0.201550  0.015504  0.031008  0.062016  \n",
              "worry     Pride          0.000000  0.200000  0.000000  0.000000  0.000000  \n",
              "hate      Pride          0.125000  0.200000  0.025000  0.100000  0.075000  \n",
              "neutral   Guilt          0.059259  0.192566  0.005620  0.015184  0.045652  \n",
              "empty     Guilt          0.067797  0.186441  0.016949  0.067797  0.084746  \n",
              "worry     Irritation     0.000000  0.181818  0.000000  0.000000  0.090909  \n",
              "relief    Guilt          0.074627  0.179104  0.014925  0.000000  0.014925  \n",
              "love      Shame          0.048257  0.160858  0.016086  0.032172  0.058981  \n",
              "hate      Boredom        0.240000  0.160000  0.000000  0.053333  0.106667  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3aafaed-a215-4ebc-983e-5b68aeb9a64e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>len_tokens</th>\n",
              "      <th>i_ratio</th>\n",
              "      <th>you_ratio</th>\n",
              "      <th>they_ratio</th>\n",
              "      <th>neg_ratio</th>\n",
              "      <th>exclam</th>\n",
              "      <th>ques</th>\n",
              "      <th>caps_ratio</th>\n",
              "      <th>past</th>\n",
              "      <th>future</th>\n",
              "      <th>moral</th>\n",
              "      <th>threat</th>\n",
              "      <th>loss</th>\n",
              "      <th>compare</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Emotion</th>\n",
              "      <th>theory_top1</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>boredom</th>\n",
              "      <th>Melancholy</th>\n",
              "      <td>38.000000</td>\n",
              "      <td>0.131579</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worry</th>\n",
              "      <th>Guilt</th>\n",
              "      <td>30.076923</td>\n",
              "      <td>0.072215</td>\n",
              "      <td>0.003205</td>\n",
              "      <td>0.003846</td>\n",
              "      <td>0.014251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <th>Guilt</th>\n",
              "      <td>26.885135</td>\n",
              "      <td>0.111495</td>\n",
              "      <td>0.006719</td>\n",
              "      <td>0.004642</td>\n",
              "      <td>0.009820</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.189189</td>\n",
              "      <td>0.060811</td>\n",
              "      <td>0.310811</td>\n",
              "      <td>0.006757</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.067568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sadness</th>\n",
              "      <th>Guilt</th>\n",
              "      <td>22.397959</td>\n",
              "      <td>0.112080</td>\n",
              "      <td>0.003088</td>\n",
              "      <td>0.003618</td>\n",
              "      <td>0.010459</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.183673</td>\n",
              "      <td>0.061224</td>\n",
              "      <td>0.275510</td>\n",
              "      <td>0.020408</td>\n",
              "      <td>0.530612</td>\n",
              "      <td>0.061224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anger</th>\n",
              "      <th>Guilt</th>\n",
              "      <td>22.467532</td>\n",
              "      <td>0.109618</td>\n",
              "      <td>0.008466</td>\n",
              "      <td>0.004560</td>\n",
              "      <td>0.008041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.168831</td>\n",
              "      <td>0.038961</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.012987</td>\n",
              "      <td>0.025974</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>happiness</th>\n",
              "      <th>Guilt</th>\n",
              "      <td>25.900585</td>\n",
              "      <td>0.118138</td>\n",
              "      <td>0.002961</td>\n",
              "      <td>0.003329</td>\n",
              "      <td>0.010116</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.172515</td>\n",
              "      <td>0.093567</td>\n",
              "      <td>0.269006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040936</td>\n",
              "      <td>0.099415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worry</th>\n",
              "      <th>Shame</th>\n",
              "      <td>28.933333</td>\n",
              "      <td>0.107037</td>\n",
              "      <td>0.007255</td>\n",
              "      <td>0.012605</td>\n",
              "      <td>0.012093</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surprise</th>\n",
              "      <th>Guilt</th>\n",
              "      <td>27.850000</td>\n",
              "      <td>0.110769</td>\n",
              "      <td>0.003590</td>\n",
              "      <td>0.007568</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hate</th>\n",
              "      <th>Guilt</th>\n",
              "      <td>25.264901</td>\n",
              "      <td>0.112590</td>\n",
              "      <td>0.008743</td>\n",
              "      <td>0.003770</td>\n",
              "      <td>0.009084</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.145695</td>\n",
              "      <td>0.092715</td>\n",
              "      <td>0.238411</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>empty</th>\n",
              "      <th>Panic</th>\n",
              "      <td>22.555556</td>\n",
              "      <td>0.097131</td>\n",
              "      <td>0.010582</td>\n",
              "      <td>0.003584</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worry</th>\n",
              "      <th>Schadenfreude</th>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.121501</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012107</td>\n",
              "      <td>0.003175</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fun</th>\n",
              "      <th>Guilt</th>\n",
              "      <td>24.449612</td>\n",
              "      <td>0.081840</td>\n",
              "      <td>0.004261</td>\n",
              "      <td>0.006329</td>\n",
              "      <td>0.014536</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.170543</td>\n",
              "      <td>0.093023</td>\n",
              "      <td>0.201550</td>\n",
              "      <td>0.015504</td>\n",
              "      <td>0.031008</td>\n",
              "      <td>0.062016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worry</th>\n",
              "      <th>Pride</th>\n",
              "      <td>33.200000</td>\n",
              "      <td>0.104345</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013462</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hate</th>\n",
              "      <th>Pride</th>\n",
              "      <td>32.900000</td>\n",
              "      <td>0.104197</td>\n",
              "      <td>0.008873</td>\n",
              "      <td>0.010459</td>\n",
              "      <td>0.008372</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.075000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <th>Guilt</th>\n",
              "      <td>19.159732</td>\n",
              "      <td>0.122145</td>\n",
              "      <td>0.003853</td>\n",
              "      <td>0.004643</td>\n",
              "      <td>0.011576</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.147900</td>\n",
              "      <td>0.059259</td>\n",
              "      <td>0.192566</td>\n",
              "      <td>0.005620</td>\n",
              "      <td>0.015184</td>\n",
              "      <td>0.045652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>empty</th>\n",
              "      <th>Guilt</th>\n",
              "      <td>26.288136</td>\n",
              "      <td>0.091703</td>\n",
              "      <td>0.004039</td>\n",
              "      <td>0.003110</td>\n",
              "      <td>0.005869</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.169492</td>\n",
              "      <td>0.067797</td>\n",
              "      <td>0.186441</td>\n",
              "      <td>0.016949</td>\n",
              "      <td>0.067797</td>\n",
              "      <td>0.084746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worry</th>\n",
              "      <th>Irritation</th>\n",
              "      <td>31.272727</td>\n",
              "      <td>0.090623</td>\n",
              "      <td>0.001855</td>\n",
              "      <td>0.005172</td>\n",
              "      <td>0.004132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relief</th>\n",
              "      <th>Guilt</th>\n",
              "      <td>27.477612</td>\n",
              "      <td>0.091336</td>\n",
              "      <td>0.006002</td>\n",
              "      <td>0.013307</td>\n",
              "      <td>0.012994</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.179104</td>\n",
              "      <td>0.074627</td>\n",
              "      <td>0.179104</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <th>Shame</th>\n",
              "      <td>27.817694</td>\n",
              "      <td>0.113335</td>\n",
              "      <td>0.011035</td>\n",
              "      <td>0.007066</td>\n",
              "      <td>0.010076</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.238606</td>\n",
              "      <td>0.048257</td>\n",
              "      <td>0.160858</td>\n",
              "      <td>0.016086</td>\n",
              "      <td>0.032172</td>\n",
              "      <td>0.058981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hate</th>\n",
              "      <th>Boredom</th>\n",
              "      <td>32.106667</td>\n",
              "      <td>0.092295</td>\n",
              "      <td>0.013706</td>\n",
              "      <td>0.003206</td>\n",
              "      <td>0.014235</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.386667</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053333</td>\n",
              "      <td>0.106667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3aafaed-a215-4ebc-983e-5b68aeb9a64e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a3aafaed-a215-4ebc-983e-5b68aeb9a64e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a3aafaed-a215-4ebc-983e-5b68aeb9a64e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "summary",
              "summary": "{\n  \"name\": \"summary\",\n  \"rows\": 829,\n  \"fields\": [\n    {\n      \"column\": \"len_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.071262870897014,\n        \"min\": 10.529411764705882,\n        \"max\": 46.0,\n        \"num_unique_values\": 809,\n        \"samples\": [\n          19.408450704225352,\n          27.10242587601078,\n          23.48993288590604\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"i_ratio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.017000397647340775,\n        \"min\": 0.0328042328042328,\n        \"max\": 0.2608695652173913,\n        \"num_unique_values\": 828,\n        \"samples\": [\n          0.1059851189492548,\n          0.11585761629741827,\n          0.11040898072425148\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"you_ratio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003904440950246008,\n        \"min\": 0.0,\n        \"max\": 0.025462536160477327,\n        \"num_unique_values\": 756,\n        \"samples\": [\n          0.0008710801393728223,\n          0.014321171358986485,\n          0.0024975024975024975\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"they_ratio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0036971502441366507,\n        \"min\": 0.0,\n        \"max\": 0.05128205128205129,\n        \"num_unique_values\": 770,\n        \"samples\": [\n          0.004291511387163561,\n          0.016693163751987282,\n          0.004693486590038314\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neg_ratio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003778671007088447,\n        \"min\": 0.0,\n        \"max\": 0.031029851761185907,\n        \"num_unique_values\": 804,\n        \"samples\": [\n          0.004837274650404322,\n          0.0024207881350738493,\n          0.0075095281008156056\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exclam\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ques\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caps_ratio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"past\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10941850589008352,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 590,\n        \"samples\": [\n          0.13284132841328414\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"future\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06980291856349481,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 536,\n        \"samples\": [\n          0.11080332409972299\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"moral\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05520954623635895,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 463,\n        \"samples\": [\n          0.031578947368421054\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"threat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04740603586378555,\n        \"min\": 0.0,\n        \"max\": 0.75,\n        \"num_unique_values\": 358,\n        \"samples\": [\n          0.005763688760806916\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14489063415917758,\n        \"min\": 0.0,\n        \"max\": 0.7659574468085106,\n        \"num_unique_values\": 439,\n        \"samples\": [\n          0.010327022375215147\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"compare\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07045857302554233,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 481,\n        \"samples\": [\n          0.021739130434782608\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "cue_cols = [\n",
        "    \"len_tokens\",\"i_ratio\",\"you_ratio\",\"they_ratio\",\"neg_ratio\",\n",
        "    \"exclam\",\"ques\",\"caps_ratio\",\"past\",\"future\",\"moral\",\"threat\",\"loss\",\"compare\"\n",
        "]\n",
        "\n",
        "cue_df = pd.DataFrame(X_cues.toarray(), columns=cue_cols)\n",
        "tmp = pd.concat([emotion_ds[[\"Emotion\",\"theory_top1\"]], cue_df], axis=1)\n",
        "\n",
        "summary = tmp.groupby([\"Emotion\",\"theory_top1\"])[cue_cols].mean().sort_values(\"moral\", ascending=False)\n",
        "summary.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6gBn8P610Vw"
      },
      "source": [
        "# 9) Build personas from mechanisms, not from emotion labels\n",
        "9A) Create a “persona feature matrix”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR51sCUt10Vw",
        "outputId": "7509909f-e0af-43ad-f8a9-3c8f545e6106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(839555, 39)\n"
          ]
        }
      ],
      "source": [
        "# Reduce the 79-d theory similarity into smaller dimensions for clustering stability\n",
        "svd = TruncatedSVD(n_components=25, random_state=42)\n",
        "S_reduced = svd.fit_transform(S)  # [N, 25]\n",
        "\n",
        "X_persona = hstack([X_cues, csr_matrix(S_reduced)]).tocsr()\n",
        "print(X_persona.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T0LHDUK10Vw"
      },
      "source": [
        "9B) Cluster into personas (MiniBatchKMeans scales well)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "XcJqMfS410Vw",
        "outputId": "a61d3198-9ef5-4413-bebe-48de9a5fef1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "persona_id\n",
              "2    170087\n",
              "4    135059\n",
              "1    128232\n",
              "6    123827\n",
              "5    110562\n",
              "3    102614\n",
              "0     47930\n",
              "7     21244\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>persona_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>170087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>135059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>128232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>123827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>110562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>102614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>47930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>21244</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "k = 8  # start with 6–10; tune later via stability + interpretability\n",
        "km = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=4096, n_init=\"auto\")\n",
        "persona_id = km.fit_predict(X_persona)\n",
        "emotion_ds[\"persona_id\"] = persona_id\n",
        "emotion_ds[\"persona_id\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sR14zbn10Vw"
      },
      "source": [
        "# 10) Make personas interpretable\n",
        "\n",
        "Name each persona using:\n",
        "\n",
        "a) top theory cards\n",
        "\n",
        "b) cue profile (agency/threat/loss/comparison)\n",
        "\n",
        "c) representative texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3ZhYTzt10Vx",
        "outputId": "e5b16147-009a-4c99-bf04-1ddaff877548"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'Feeling like a Fruad': 1894,\n",
              "  'Hopefulness': 1532,\n",
              "  'Awumbuk': 1422,\n",
              "  'Desire': 1378,\n",
              "  'Confidence': 1324,\n",
              "  'Comfort': 1248,\n",
              "  'Fago': 1230},\n",
              " ['i may go through different stages of grief it may take a while to really feel a strong sense of healing and i know that there will never be a time when i dont miss this person'])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "def top_items(series, n=5):\n",
        "    return series.value_counts().head(n).to_dict()\n",
        "\n",
        "persona_reports = []\n",
        "for pid in sorted(emotion_ds[\"persona_id\"].unique()):\n",
        "    sub = emotion_ds[emotion_ds[\"persona_id\"] == pid]\n",
        "\n",
        "    # Top theory lenses\n",
        "    top_theory = top_items(sub[\"theory_top1\"], n=7)\n",
        "\n",
        "    # Cue means\n",
        "    cue_means = cue_df.loc[sub.index].mean().to_dict()\n",
        "\n",
        "    # Representative examples (highest top1 score)\n",
        "    ex = sub.sort_values(\"theory_top1_score\", ascending=False).head(3)[\"text\"].tolist()\n",
        "\n",
        "    persona_reports.append({\n",
        "        \"persona_id\": pid,\n",
        "        \"size\": len(sub),\n",
        "        \"top_theory_cards\": top_theory,\n",
        "        \"cue_means\": cue_means,\n",
        "        \"examples\": ex\n",
        "    })\n",
        "\n",
        "persona_reports[0][\"top_theory_cards\"], persona_reports[0][\"examples\"][:1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Visualization of Personas"
      ],
      "metadata": {
        "id": "50KUJ7DC1INW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "'''def persona_card(pid, n_examples=2):\n",
        "    sub = emotion_ds[emotion_ds[\"persona_id\"] == pid]\n",
        "    size = len(sub)\n",
        "    pct = size / len(emotion_ds)\n",
        "\n",
        "    top_cards = sub[\"theory_top1\"].value_counts().head(5)\n",
        "    top_cards_str = \", \".join([f\"{k} ({v})\" for k, v in top_cards.items()])\n",
        "\n",
        "    # cue z-scores for this persona: show strongest + and -\n",
        "    z = cue_z.loc[pid].sort_values(ascending=False)\n",
        "    top_pos = \", \".join([f\"{k}:{z[k]:.2f}\" for k in z.index[:4]])\n",
        "    top_neg = \", \".join([f\"{k}:{z[k]:.2f}\" for k in z.index[-4:]])\n",
        "\n",
        "    ex = sub.sort_values(\"theory_top1_score\", ascending=False).head(n_examples)[\"text\"].tolist()\n",
        "\n",
        "    print(f\"\\n=== Persona {pid+1} ===\")\n",
        "    print(f\"Size: {size} ({pct:.1%})\")\n",
        "    print(f\"Top theory cards: {top_cards_str}\")\n",
        "    print(f\"Cue highs (z): {top_pos}\")\n",
        "    print(f\"Cue lows  (z): {top_neg}\")\n",
        "    print(\"Examples:\")\n",
        "    for i, t in enumerate(ex, 1):\n",
        "        print(f\"  {i}) {t[:250]}{'...' if len(t) > 250 else ''}\")\n",
        "\n",
        "for pid in sorted(emotion_ds[\"persona_id\"].unique()):\n",
        "    persona_card(pid, n_examples=2)'''\n",
        "def persona_card(pid, n_examples=2):\n",
        "    sub = emotion_ds[emotion_ds[\"persona_id\"] == pid]\n",
        "    size = len(sub)\n",
        "    pct = size / len(emotion_ds)\n",
        "\n",
        "    top_cards = sub[\"theory_top1\"].value_counts().head(5)\n",
        "    top_cards_str = \", \".join([f\"{k} ({v})\" for k, v in top_cards.items()])\n",
        "\n",
        "    z = cue_z.loc[pid].sort_values(ascending=False)\n",
        "    top_pos = \", \".join([f\"{k}:{z[k]:.2f}\" for k in z.index[:4]])\n",
        "    top_neg = \", \".join([f\"{k}:{z[k]:.2f}\" for k in z.index[-4:]])\n",
        "\n",
        "    ex = (\n",
        "        sub.sort_values(\"theory_top1_score\", ascending=False)\n",
        "           .drop_duplicates(subset=[\"text\"])\n",
        "           .head(n_examples)[\"text\"]\n",
        "           .tolist()\n",
        "    )\n",
        "\n",
        "    print(f\"\\n=== Persona {pid+1} ===\")\n",
        "    print(f\"Size: {size} ({pct:.1%})\")\n",
        "    print(f\"Top theory cards: {top_cards_str}\")\n",
        "    print(f\"Cue highs (z): {top_pos}\")\n",
        "    print(f\"Cue lows  (z): {top_neg}\")\n",
        "    print(\"Examples:\")\n",
        "    for i, t in enumerate(ex, 1):\n",
        "        print(f\"  {i}) {t[:250]}{'...' if len(t) > 250 else ''}\")\n",
        "\n",
        "for pid in sorted(emotion_ds[\"persona_id\"].unique()):\n",
        "    persona_card(pid, n_examples=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHM27bNj1OMY",
        "outputId": "b0e6f0de-a4c7-42c5-9817-57c1279eb6d3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Persona 1 ===\n",
            "Size: 47930 (5.7%)\n",
            "Top theory cards: Feeling like a Fruad (1894), Hopefulness (1532), Awumbuk (1422), Desire (1378), Confidence (1324)\n",
            "Cue highs (z): len_tokens:1.95, past:0.41, future:0.38, compare:0.26\n",
            "Cue lows  (z): ques:0.00, exclam:0.00, caps_ratio:0.00, i_ratio:-0.48\n",
            "Examples:\n",
            "  1) i may go through different stages of grief it may take a while to really feel a strong sense of healing and i know that there will never be a time when i dont miss this person\n",
            "  2) i said when thanking you last fall i feel a curious combination of pride and humility pride at having started this but humility at the constant reminder that this project is after all staffed by self motivated volunteers\n",
            "\n",
            "=== Persona 2 ===\n",
            "Size: 128232 (15.3%)\n",
            "Top theory cards: Feeling like a Fruad (7210), Comfort (5099), Hopefulness (4075), Awumbuk (3794), Confidence (3655)\n",
            "Cue highs (z): neg_ratio:0.02, exclam:0.00, caps_ratio:0.00, ques:0.00\n",
            "Cue lows  (z): compare:-0.04, past:-0.05, future:-0.06, len_tokens:-0.34\n",
            "Examples:\n",
            "  1) i feel when they are distressed in the night is perhaps more than empathy\n",
            "  2) i get the feeling that there is still a bunch of repressed rage about\n",
            "\n",
            "=== Persona 3 ===\n",
            "Size: 170087 (20.3%)\n",
            "Top theory cards: Comfort (31389), Feeling like a Fruad (21799), Empathy (4508), Warm Glow (4106), Hopefulness (3640)\n",
            "Cue highs (z): i_ratio:0.59, exclam:0.00, caps_ratio:0.00, ques:0.00\n",
            "Cue lows  (z): compare:-0.17, future:-0.24, past:-0.29, len_tokens:-1.13\n",
            "Examples:\n",
            "  1) loneliness\n",
            "  2) the same as in guilt\n",
            "\n",
            "=== Persona 4 ===\n",
            "Size: 102614 (12.2%)\n",
            "Top theory cards: Feeling like a Fruad (4131), Hopefulness (3494), Awumbuk (3074), Desire (2957), Confidence (2695)\n",
            "Cue highs (z): len_tokens:1.10, past:0.26, future:0.22, compare:0.16\n",
            "Cue lows  (z): ques:0.00, exclam:0.00, caps_ratio:0.00, i_ratio:-0.38\n",
            "Examples:\n",
            "  1) i feel about any other part of pride i still believe in what dykes on bikes stand for and am still proud to lead the pride march because of it\n",
            "  2) i feel that if we re going to celebrate youth pride if we re going to show our pride we should do it in a dignified way satre says\n",
            "\n",
            "=== Persona 5 ===\n",
            "Size: 135059 (16.1%)\n",
            "Top theory cards: Feeling like a Fruad (10115), Comfort (9250), Hopefulness (4127), Confidence (3753), Love (3699)\n",
            "Cue highs (z): i_ratio:0.17, exclam:0.00, caps_ratio:0.00, ques:0.00\n",
            "Cue lows  (z): compare:-0.09, future:-0.14, past:-0.17, len_tokens:-0.70\n",
            "Examples:\n",
            "  1) i am and i feel a sympathetic embarrassment for them\n",
            "  2) i am feeling very violent and almost in constant rage\n",
            "\n",
            "=== Persona 6 ===\n",
            "Size: 110562 (13.2%)\n",
            "Top theory cards: Feeling like a Fruad (5099), Hopefulness (3742), Awumbuk (3413), Comfort (3378), Confidence (3199)\n",
            "Cue highs (z): past:0.03, they_ratio:0.03, len_tokens:0.02, neg_ratio:0.02\n",
            "Cue lows  (z): caps_ratio:0.00, threat:-0.00, loss:-0.01, i_ratio:-0.15\n",
            "Examples:\n",
            "  1) i feel i would hate to think that everyone on our freeways has only two emotions rage and repressed rage\n",
            "  2) i dress to express personal pride the more pride i feel and there s nothing vicious about that circle\n",
            "\n",
            "=== Persona 7 ===\n",
            "Size: 123827 (14.7%)\n",
            "Top theory cards: Feeling like a Fruad (5422), Hopefulness (4002), Awumbuk (3827), Confidence (3384), Comfort (3372)\n",
            "Cue highs (z): len_tokens:0.46, past:0.14, future:0.11, they_ratio:0.07\n",
            "Cue lows  (z): ques:0.00, exclam:0.00, caps_ratio:0.00, i_ratio:-0.26\n",
            "Examples:\n",
            "  1) i feel that caring can only come about through empathy and hope provide an understanding and empathy of world through my art\n",
            "  2) i have seen even more clearly how pride ensnared me and i do feel much more free from it now though it is still there for sure\n",
            "\n",
            "=== Persona 8 ===\n",
            "Size: 21244 (2.5%)\n",
            "Top theory cards: Hopefulness (781), Feeling like a Fruad (764), Awumbuk (700), Confidence (659), Desire (636)\n",
            "Cue highs (z): len_tokens:2.97, past:0.54, future:0.51, compare:0.38\n",
            "Cue lows  (z): ques:0.00, exclam:0.00, caps_ratio:0.00, i_ratio:-0.49\n",
            "Examples:\n",
            "  1) i feel ashamed now cos i can see deep in me nothing rage i feel ashamed now cos i can see deep in you nothing i do believe my greed is deeper than your love rage i feel ashamed now cos i can see deep in me nothing rage i feel ashamed now cos i can se...\n",
            "  2) i spill something or drop a dish that breaks it s a moment of terror that has to be taken care of right away because i still feel that hot terror that i felt when i was getting screamed at and mom was taking out her frustration on my face\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wrhZfDm939FB"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}